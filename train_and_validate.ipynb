{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5774ddea",
   "metadata": {},
   "source": [
    "# Training and validation notebook\n",
    "\n",
    "This notebook trains a small diffusion denoiser on synthetic degradation traces and saves a checkpoint (`checkpoints/best.pth`) and an optional safetensors copy.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391c8d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Imports and device selection\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa126589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small synthetic generator (Paris-law) - extracted from code\n",
    "import numpy as _np\n",
    "class ParisLawDegradation:\n",
    "    def __init__(self, length, dim, C=1e-12, m=3, delta_sigma=100, beta=1):\n",
    "        self.length = int(length)\n",
    "        self.dim = int(dim)\n",
    "        self.C = float(C)\n",
    "        self.m = float(m)\n",
    "        self.delta_sigma = float(delta_sigma)\n",
    "        self.beta = float(beta)\n",
    "    def delta_K(self, a):\n",
    "        a = _np.atleast_1d(_np.asarray(a))\n",
    "        return self.delta_sigma * _np.sqrt(_np.pi * a) * self.beta\n",
    "    def xdot(self, a):\n",
    "        a = _np.atleast_1d(_np.asarray(a))\n",
    "        return self.C * (self.delta_K(a) ** self.m)\n",
    "    def generate_episode(self, x0):\n",
    "        x0 = _np.atleast_1d(_np.asarray(x0))\n",
    "        episode = _np.zeros((x0.shape[0], self.length + 1))\n",
    "        episode[:, 0] = x0\n",
    "        for i in range(self.length):\n",
    "            episode[:, i + 1] = episode[:, i] + self.xdot(episode[:, i])\n",
    "        return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94dc2690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape torch.Size([1024, 1, 64])\n"
     ]
    }
   ],
   "source": [
    "# Build a small dataset for training (quick)\n",
    "N = 1024\n",
    "length = 64\n",
    "gen = ParisLawDegradation(length=length-1, dim=1, C=1e-8)\n",
    "x0 = _np.abs(_np.random.randn(N)) * 1e-3 + 1e-4\n",
    "episodes = gen.generate_episode(x0)\n",
    "# clean\n",
    "episodes = episodes[~_np.isnan(episodes).any(axis=1)]\n",
    "# to torch and normalize per-sample min/max\n",
    "data = torch.tensor(episodes, dtype=torch.float32).to(device)\n",
    "X = data[:, None, ...]  # (N, C, L)\n",
    "mn = torch.min(X, 2)[0][..., None]\n",
    "mx = torch.max(X, 2)[0][..., None]\n",
    "den = (mx - mn)\n",
    "den[den == 0] = 1.0\n",
    "X = (X - mn) / den\n",
    "print('X shape', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2c0b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesDiffusionModel(\n",
      "  (input_proj): Linear(in_features=1, out_features=32, bias=True)\n",
      "  (time_emb): TimeEmbedding(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_proj): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Import models from src package\n",
    "from src.degdiff.model_def import TimeSeriesDiffusionModel, DegDiffusion\n",
    "# instantiate a small model\n",
    "C = X.shape[1]\n",
    "L = X.shape[2]\n",
    "model = TimeSeriesDiffusionModel(channels=C, hidden_dim=32, num_blocks=1, T=50).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfad770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.653086\n",
      "Epoch 1, Loss 0.645431\n"
     ]
    }
   ],
   "source": [
    "# Minimal training loop with saving of a checkpoint at the end\n",
    "bs = 16\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "s0_len = L // 2\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    idx = torch.randint(0, X.shape[0], (bs,))\n",
    "    x0 = X[idx].to(device)\n",
    "    t = torch.randint(0, model.T, (bs,)).to(device)\n",
    "    noise = torch.randn_like(x0).to(device)\n",
    "    xt = model.q_sample(x0, t, noise)\n",
    "    s0 = x0[:, :, :s0_len]\n",
    "    ns1 = xt[:, :, s0_len:]\n",
    "    noise1 = noise[:, :, s0_len:]\n",
    "    noise0 = torch.zeros_like(s0).to(device)\n",
    "    output_noise = torch.cat([noise0, noise1], dim=2)\n",
    "    pred_noise = model(s0, ns1, t)\n",
    "    loss = loss_fn(pred_noise, output_noise)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Loss {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c99dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to checkpoints/best.pth\n"
     ]
    }
   ],
   "source": [
    "# Save checkpoint after training\n",
    "import os\n",
    "ckpt_dir = 'checkpoints'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "ckpt_path = os.path.join(ckpt_dir, 'best.pth')\n",
    "torch.save({'state_dict': model.state_dict(), 'epoch': epoch, 'metrics': {'loss': loss.item()}}, ckpt_path)\n",
    "print(f'Saved checkpoint to {ckpt_path}')\n",
    "# Optional safetensors copy\n",
    "try:\n",
    "    from safetensors.torch import save_file as safe_save\n",
    "    safe_save({k: v.cpu().numpy() for k, v in model.state_dict().items()}, os.path.join(ckpt_dir, 'best.safetensors'))\n",
    "    print('Saved safetensors copy')\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46804b68",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "- Verify `checkpoints/best.pth` is created.\n",
    "- Locally test publishing: `PYTHONPATH=src MODEL_CHECKPOINT=checkpoints/best.pth HF_TOKEN=... python scripts/publish_to_hf.py`\n",
    "- Add CI workflow to upload the artifact from training job and run the publisher in a second job."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartchp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
